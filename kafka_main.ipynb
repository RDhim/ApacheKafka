{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "kafka-topics.sh --create --topic sensor-suhu --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install kafka-python\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulasi Data Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "sensor_ids = ['S1', 'S2', 'S3']\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        for sensor_id in sensor_ids:\n",
    "            suhu = random.randint(60, 100)\n",
    "            data = {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"suhu\": suhu\n",
    "            }\n",
    "            producer.send('sensor-suhu', value=data)\n",
    "            print(f\"Data terkirim: {data}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Producer berhenti.\")\n",
    "finally:\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olah Data Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Inisialisasi Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SensorSuhuConsumer\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definisikan skema data yang diterima\n",
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"suhu\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Membaca data dari Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"sensor-suhu\") \\\n",
    "    .load()\n",
    "\n",
    "# Ubah data dari format biner ke JSON\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "df = df.select(from_json(col(\"json\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Filter data dengan suhu di atas 80°C\n",
    "filtered_df = df.filter(col(\"suhu\") > 80)\n",
    "\n",
    "# Cetak data suhu di atas 80°C ke console sebagai tanda peringatan\n",
    "query = filtered_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
